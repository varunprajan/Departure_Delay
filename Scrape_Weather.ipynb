{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directory to Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SUBDIR = 'data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Code(object):\n",
    "    \"\"\"Class for determining if an airport or station code is legitimate\"\"\"\n",
    "    def __init__(self,length):\n",
    "        self.length = length\n",
    "        \n",
    "    def is_legit_code(self,code):\n",
    "        return (not code[0].isdigit()) and (len(code) == self.length)\n",
    "\n",
    "class StationCodesFile(object):\n",
    "    \"\"\"Class for reading airport and station codes and producing the corresponding mapping between the two\"\"\"\n",
    "    def __init__(self,filename,headerlines,startcol,\n",
    "                 subdir=SUBDIR,airportcode=None,stationcode=None):\n",
    "        self.filepath = os.path.join(subdir,filename)\n",
    "        self.headerlines = headerlines\n",
    "        self.startcol = startcol\n",
    "        self.airportcode = Code(3) if airportcode is None else airportcode # airport codes are three chars long\n",
    "        self.stationcode = Code(4) if stationcode is None else stationcode # weather station codes are four chars\n",
    "        \n",
    "    def yield_station_codes(self):\n",
    "        with open(self.filepath,'r') as f:\n",
    "            for _ in range(self.headerlines):\n",
    "                f.next()\n",
    "            for line in f:\n",
    "                line = line[self.startcol:]\n",
    "                words = line.split()\n",
    "                if len(words) >= 2:\n",
    "                    airport, station = words[1], words[0]\n",
    "                    if self.airportcode.is_legit_code(airport) and self.stationcode.is_legit_code(station):\n",
    "                        yield airport, station\n",
    "                        \n",
    "    def code_mapping(self):\n",
    "        return {airport: station for airport, station in self.yield_station_codes()}\n",
    "    \n",
    "class WundergroundScraper(object):\n",
    "    \"\"\"Class for scraping weather from wunderground.com\"\"\"\n",
    "    def __init__(self,stationdict):\n",
    "        self.url_template = 'https://www.wunderground.com/history/airport/{0}/{1}/{2}/{3}/DailyHistory.html'\n",
    "        self.funcdict = {'TEMP': self.get_actual_temp,\n",
    "                         'PRECIP': self.get_actual_precip}\n",
    "        self.stationdict = stationdict\n",
    "        \n",
    "    def get_station(self,airport):\n",
    "        \"\"\"Get weather station corresponding to an airport\"\"\"\n",
    "        if airport in self.stationdict:\n",
    "            return self.stationdict[airport]\n",
    "        else:\n",
    "            print 'Bad airport'\n",
    "            raise ValueError\n",
    "        \n",
    "    def scrape_from_date_and_airport(self,date,airport,attrs):\n",
    "        \"\"\"Scrape data for a specific date and location (airport)\"\"\"\n",
    "        station = self.get_station(airport)\n",
    "        soup = self.soupify_date_station(date,station)\n",
    "        res = {}\n",
    "        for attr in attrs:\n",
    "            func = self.funcdict[attr]\n",
    "            res[attr] = func(soup)\n",
    "        return res\n",
    "\n",
    "    def soupify_date_station(self,date,station):\n",
    "        \"\"\"Get beautiful soup representation of weather\"\"\"\n",
    "        url = self.url_template.format(station,date.year,date.month,date.day)\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text,'html.parser')\n",
    "        return soup\n",
    "    \n",
    "    def extract_tagged_value(self,soup,tag,idxcol):\n",
    "        \"\"\"Extract a tagged value from a Wunderground table\"\"\"\n",
    "        for tr in soup.find_all('tr'):\n",
    "            if tr.find('td',text=tag) is not None:\n",
    "                vals = tr.find_all('span',{'class': 'wx-value'})\n",
    "                if vals:\n",
    "                    return vals[idxcol].contents[0]\n",
    "    \n",
    "    def get_actual_temp(self,soup,idxcol=0):\n",
    "        \"\"\"Get the mean, actual temperature from the soup\"\"\"\n",
    "        temp = self.extract_tagged_value(soup,'Mean Temperature',idxcol)\n",
    "        try:\n",
    "            return float(temp)\n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "            \n",
    "    def get_actual_precip(self,soup,idxcol=0):\n",
    "        \"\"\"Get the actual precipitation from the soup\"\"\"\n",
    "        precip = self.extract_tagged_value(soup,'Precipitation',idxcol)\n",
    "        try:\n",
    "            return float(precip)\n",
    "        except (ValueError, TypeError):\n",
    "            try:\n",
    "                if precip.startswith('T'): # trace precipitation\n",
    "                    return 0.0\n",
    "                else:\n",
    "                    return None\n",
    "            except AttributeError:\n",
    "                return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_all_weather(wscraper,startdate,enddate,airports,attrs,delay=1):\n",
    "    \"\"\"Loop over airports and date, scrape the weather for each, and save to csv file periodicially\"\"\"\n",
    "    datecurr = startdate\n",
    "    res = []\n",
    "    columns = ['Date','Airport'] + attrs\n",
    "    while datecurr <= enddate:\n",
    "        for airport in airports:\n",
    "            try:\n",
    "                rescurr = wscraper.scrape_from_date_and_airport(datecurr,airport,attrs)\n",
    "            except ValueError:\n",
    "                print \"Failed for airport {0} and date {1}\".format(airport,datecurr)\n",
    "                rescurr = {attr: None for attr in attrs}\n",
    "            res.append([datecurr,airport] + [rescurr[attr] for attr in attrs])\n",
    "            time.sleep(delay)\n",
    "        datecurr += timedelta(days=1)\n",
    "    return pd.DataFrame(res,columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Scraper Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stationcodes = StationCodesFile(filename='station_codes.txt',headerlines=41,startcol=20)\n",
    "stationdict = stationcodes.code_mapping()\n",
    "wscraper = WundergroundScraper(stationdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unique_airports = np.load(os.path.join(SUBDIR,'airports.npy')) # load all the unique airports in the flight data\n",
    "dfweather = scrape_all_weather(wscraper=wscraper,\n",
    "                               startdate = pd.to_datetime('11/01/2013'),\n",
    "                               enddate = pd.to_datetime('11/30/2013'),\n",
    "                               airports = unique_airports,\n",
    "                               attrs = ['TEMP','PRECIP'],\n",
    "                               delay = 0.1) # scrape each of them, from Nov. 1 - Nov. 30\n",
    "dfweather.to_csv(os.path.join(SUBDIR,'weather.csv'),encoding='utf-8') # save to csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
